{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "T8eyBfDpludj",
   "metadata": {
    "id": "T8eyBfDpludj"
   },
   "source": [
    "# The Particle Transformer\n",
    "\n",
    "7/1/23\n",
    "\n",
    "Summary notebook for the Particle Transformer, including all necessary functions and classes for running.\n",
    "\n",
    "Importing all necessary libaries for implementation and training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd32fb0",
   "metadata": {
    "id": "cfd32fb0",
    "outputId": "17ca4eb1-d83c-475e-ce6b-76472d875a10"
   },
   "outputs": [],
   "source": [
    "%cd 'data location'/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47e05e",
   "metadata": {
    "id": "db47e05e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from functools import partial\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import vector\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "from kerasPMHA import KerasPMHA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SF2eZ0JYmPdf",
   "metadata": {
    "id": "SF2eZ0JYmPdf"
   },
   "source": [
    "Defining a function to take a training history input and return the training curves (accuracy and loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b733c85",
   "metadata": {
    "id": "9b733c85"
   },
   "outputs": [],
   "source": [
    "def histplot(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    epochs = hist.index.to_numpy() +1\n",
    "    fig = make_subplots(rows=1, cols=2,subplot_titles=('Accuracy',  'Loss'))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(mode=\"markers+lines\", x=epochs, y=hist[\"Accuracy\"], name = \"Accuracy\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(mode=\"markers+lines\", x=epochs, y=hist[\"val_Accuracy\"], name = \"Val accuracy\"), row=1, col=1)\n",
    "    \n",
    "    fig.add_shape(type='line',\n",
    "                x0=0,\n",
    "                y0=np.max(hist[\"val_Accuracy\"]),\n",
    "                x1=np.max(epochs),\n",
    "                y1=np.max(hist[\"val_Accuracy\"]),\n",
    "                line=dict(color='Green',dash=\"dot\"),\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                row=1,\n",
    "                col=1,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(go.Scatter(mode=\"markers+lines\", x=epochs, y=hist[\"loss\"], name = \"Loss\"), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(mode=\"markers+lines\", x=epochs, y=hist[\"val_loss\"], name = \"Val loss\"), row=1, col=2)\n",
    "    \n",
    "    fig.add_shape(type='line',\n",
    "                x0=0,\n",
    "                y0=np.min(hist[\"val_loss\"]),\n",
    "                x1=np.max(epochs),\n",
    "                y1=np.min(hist[\"val_loss\"]),\n",
    "                line=dict(color='Green',dash=\"dot\"),\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                row=1,\n",
    "                col=2,\n",
    "    )\n",
    "    \n",
    "    fig['layout']['xaxis']['title']='Epoch'\n",
    "    fig['layout']['xaxis2']['title']='Epoch'\n",
    "    fig['layout']['yaxis']['title']='Accuracy'\n",
    "    fig['layout']['yaxis2']['title']='Loss'\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BFDQaYZumb_0",
   "metadata": {
    "id": "BFDQaYZumb_0"
   },
   "source": [
    "## Data importing and preprocessing\n",
    "\n",
    "Defining the functions to calcuate and pairwise kinematics. A modulo function is also defined as training runs into errors using the default implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb753cee",
   "metadata": {
    "id": "cb753cee"
   },
   "outputs": [],
   "source": [
    "def tensor_mod(a, n):\n",
    "    a = tf.cast(a, dtype=tf.float32)\n",
    "    n = tf.cast(n, dtype=tf.float32)\n",
    "    \n",
    "    return a - tf.multiply(n, tf.math.floor(a/n))\n",
    "\n",
    "def delta_phi(a, b):\n",
    "    x = tf.subtract(a, b)\n",
    "    x += np.pi\n",
    "    n = tf.constant(2*np.pi)\n",
    "    x = tensor_mod(x, n)\n",
    "    x -=np.pi\n",
    "    return x\n",
    "\n",
    "\n",
    "def delta_r2(eta1, phi1, eta2, phi2):\n",
    "    return tf.square(eta1 - eta2) + tf.square(delta_phi(phi1, phi2))\n",
    "\n",
    "def to_pt2(x, eps=1e-8):\n",
    "    pt2 = tf.reduce_sum(tf.square(x[:, :2]), axis=1, keepdims=True)\n",
    "    if eps is not None:\n",
    "        pt2 = tf.clip_by_value(pt2, clip_value_min=eps, clip_value_max=10e32)\n",
    "    return pt2\n",
    "\n",
    "def atan2(y, x):\n",
    "    sx = tf.math.sign(x)\n",
    "    sy = tf.math.sign(y)\n",
    "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
    "    atan_part = tf.math.atan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
    "    return atan_part + pi_part\n",
    "\n",
    "\n",
    "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    px, py, pz, energy = tf.split(x, (1, 1, 1, 1), axis=1)\n",
    "    pt = tf.sqrt(to_pt2(x, eps=eps))\n",
    "    rapidity = 0.5 * tf.math.log(1 + (2 * pz) / (energy - pz))\n",
    "    sign = tf.math.sign(rapidity)\n",
    "    rapidity =  tf.clip_by_value(tf.abs(rapidity), clip_value_min=1e-20, clip_value_max=10e32)\n",
    "    rapidity = sign*rapidity\n",
    "    \n",
    "    phi = (atan2 if for_onnx else tf.math.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return tf.concat((pt, rapidity, phi), axis=1)\n",
    "    else:\n",
    "        m = tf.sqrt(to_m2(x, eps=eps))\n",
    "        return tf.concat((pt, rapidity, phi, m), axis=1)\n",
    "\n",
    "\n",
    "def boost(x, boostp4, eps=1e-8):\n",
    "    # boost x to the rest frame of boostp4\n",
    "    p3 = tf.clip_by_value(-boostp4[:, :3] / boostp4[:, 3:], clip_value_min=eps, clip_value_max=10e32)\n",
    "    b2 = tf.reduce_sum(tf.square(p3), axis = 1, keepdims=True)\n",
    "    gamma = tf.sqrt(tf.clip_by_value(1 - b2, clip_value_min=eps, clip_value_max=10e32))\n",
    "    gamma2 = (gamma - 1) / b2\n",
    "    gamma2 = tf.where(b2==0, 0, gamma2)\n",
    "\n",
    "    bp = tf.reduce_sum(x[:, :3] * p3, axis = 1, keepdims=True)\n",
    "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
    "    return v\n",
    "\n",
    "\n",
    "def p3_norm(p, eps=1e-8):\n",
    "    return tf.clip_by_value(p[:, :3] / tf.norm(p[:, :3], axis=1, keepdims=True), clip_value_min=eps, clip_value_max=10e32)\n",
    "\n",
    "def to_m2(x, eps=1e-8):\n",
    "    m2 = tf.square(x[:, :, 3:4]) - tf.reduce_sum(tf.square(x[:, :, :3]), axis=-1, keepdims=True)\n",
    "    if eps is not None:\n",
    "        m2 = tf.clip_by_value(m2, clip_value_min=eps, clip_value_max=10e32)\n",
    "    return m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8EI8VJe6mwjt",
   "metadata": {
    "id": "8EI8VJe6mwjt"
   },
   "source": [
    "Defining the import funtion for top data, and functions to calculate single particle transverse momentum, rapidity, and phi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d34708",
   "metadata": {
    "id": "e3d34708"
   },
   "outputs": [],
   "source": [
    "def to_pt2_pre(x, eps=1e-8):\n",
    "    pt2 = tf.reduce_sum(tf.square(x[:, :, :2]), axis=2, keepdims=True)\n",
    "    if eps is not None:\n",
    "        pt2 = tf.clip_by_value(pt2, clip_value_min=eps, clip_value_max=10e32)\n",
    "    return pt2\n",
    "\n",
    "def to_ptrapphim_pre(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    px, py, pz, energy = tf.split(x, (1, 1, 1, 1), axis=2)\n",
    "    pt = tf.sqrt(to_pt2_pre(x, eps=eps))\n",
    "    \n",
    "    rapidity = 0.5 * tf.math.log(1 + (2 * pz) / (energy - pz))\n",
    "    sign = tf.math.sign(rapidity)\n",
    "    rapidity =  tf.clip_by_value(tf.abs(rapidity), clip_value_min=1e-20, clip_value_max=10e32)\n",
    "    rapidity = sign*rapidity\n",
    "    \n",
    "    phi = (atan2 if for_onnx else tf.math.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return pt, rapidity, phi\n",
    "    else:\n",
    "        m = tf.sqrt(to_m2(x, eps=eps))\n",
    "        return tf.concat((pt, rapidity, phi, m), axis=1)\n",
    "\n",
    "def import_top_data(filename, dataset_size, alpha = 0., eps=1e-8, padding=200):\n",
    "    print(\"Importing data...\\n\")\n",
    "    path = \"TopTagging/{}.h5\".format(filename)\n",
    "    df = pd.read_hdf(path, 'table')\n",
    "    \n",
    "    part_px = []\n",
    "    part_py = []\n",
    "    part_pz = []\n",
    "    part_E = []\n",
    "\n",
    "    if padding > 200: padding=200\n",
    "    \n",
    "    for i in range(padding):\n",
    "        part_px.append(df[\"PX_{}\".format(i)][:dataset_size].to_list())\n",
    "        part_py.append(df[\"PY_{}\".format(i)][:dataset_size].to_list())\n",
    "        part_pz.append(df[\"PZ_{}\".format(i)][:dataset_size].to_list())\n",
    "        part_E.append(df[\"E_{}\".format(i)][:dataset_size].to_list())\n",
    "\n",
    "\n",
    "    rot_mat = [[1.,0.,0.,0.],\n",
    "           [0.,1.,0.,0.],\n",
    "           [0.,0.,np.cos(alpha), -np.sin(alpha)],\n",
    "           [0.,0.,np.sin(alpha), np.cos(alpha)]]\n",
    "    rot_tensor = tf.constant(rot_mat)\n",
    "    \n",
    "    \n",
    "    part_px = np.stack(part_px, axis=-1)\n",
    "    part_py = np.stack(part_py, axis=-1)\n",
    "    part_pz = np.stack(part_pz, axis=-1)\n",
    "    part_E = np.stack(part_E, axis=-1)   \n",
    "     \n",
    "    if alpha != 0:\n",
    "        part_py_p = np.cos(alpha)*part_py - np.sin(alpha)*part_pz\n",
    "        part_pz_p = np.cos(alpha)*part_pz + np.sin(alpha)*part_py\n",
    "        part_py = part_py_p\n",
    "        part_pz = part_pz_p\n",
    "    \n",
    "    jet_px = np.sum(np.asarray(part_px),axis=1)\n",
    "    jet_py= np.sum(np.asarray(part_py),axis=1)\n",
    "    jet_pz= np.sum(np.asarray(part_pz),axis=1)\n",
    "    jet_E= np.sum(np.asarray(part_E),axis=1)        \n",
    "        \n",
    "    batch_part = tf.stack([part_px, part_py, part_pz, part_E], axis=-1)\n",
    "    batch_jet = tf.stack([jet_px, jet_py, jet_pz, jet_E], axis=-1)\n",
    "    \n",
    "    batch_jet = tf.reshape(batch_jet, (tf.shape(batch_jet)[0],1,tf.shape(batch_jet)[1]))\n",
    "    batch_jet = tf.tile(batch_jet, (1,padding, 1))\n",
    "\n",
    "    part_pt, part_rapidity, part_phi = to_ptrapphim_pre(batch_part, return_mass=False, eps=1e-8, for_onnx=False)\n",
    "    jet_pt, jet_rapidity, jet_phi= to_ptrapphim_pre(batch_jet, return_mass=False, eps=1e-8, for_onnx=False)\n",
    "    \n",
    "    jet_E = tf.reshape(jet_E, (tf.shape(jet_E)[0],1))\n",
    "    jet_E = tf.tile(jet_E, (1,padding))\n",
    "\n",
    "    part_eta = part_rapidity - jet_rapidity\n",
    "    part_phi = delta_phi(part_phi, jet_phi)\n",
    "    part_logpt = tf.clip_by_value(tf.math.log(part_pt), clip_value_min=eps, clip_value_max=10e32)\n",
    "    part_logE = tf.math.sign(tf.math.log(part_E))*(tf.clip_by_value(tf.abs(tf.math.log(part_E)), clip_value_min=-eps, clip_value_max=10e32))\n",
    "    part_ptptjet =  part_pt/jet_pt \n",
    "    part_EEjet =  part_E/jet_E \n",
    "    part_dR = tf.sqrt(tf.square(tf.cast(part_eta,dtype=tf.float32)) + tf.square(tf.cast(part_phi,dtype=tf.float32)))\n",
    "    y = df['is_signal_new'][:dataset_size].to_list()\n",
    "    \n",
    "    batch = np.stack([part_px, part_py, part_pz, part_E, tf.squeeze(part_eta), tf.squeeze(part_phi), tf.squeeze(part_logpt),\n",
    "                           tf.squeeze(part_logE), tf.squeeze(part_ptptjet), tf.squeeze(part_EEjet), tf.squeeze(part_dR)], axis=-1)\n",
    "    \n",
    "    i=0\n",
    "\n",
    "    tf_batch = tf.reshape(tf.convert_to_tensor(()), (0, padding, 11))\n",
    "    tf_batch = tf.cast(tf_batch,dtype=tf.float32)\n",
    "    while (i+1)*100000 < dataset_size:\n",
    "        sub_batch = tf.convert_to_tensor(batch[i*100000:(i+1)*100000])\n",
    "        padding_bool = tf.reduce_all(sub_batch[:,:,:4]!=[0.,0.,0.,0.],axis=-1) \n",
    "        padding_arr = tf.cast(padding_bool, dtype=tf.float64)\n",
    "        sub_batch = tf.einsum('...i,...ij->...ij', padding_arr, sub_batch)\n",
    "        sub_batch = tf.where(tf.math.is_nan(sub_batch), tf.zeros_like(sub_batch), sub_batch)\n",
    "        sub_batch = tf.cast(sub_batch,dtype=tf.float32)\n",
    "        tf_batch = tf.concat([tf_batch, sub_batch],0)\n",
    "        i+=1\n",
    "    sub_batch = tf.convert_to_tensor(batch[i*100000:])\n",
    "    padding_bool = tf.reduce_all(sub_batch[:,:,:4]!=[0.,0.,0.,0.],axis=-1) \n",
    "    padding_arr = tf.cast(padding_bool, dtype=tf.float64)\n",
    "    sub_batch = tf.einsum('...i,...ij->...ij', padding_arr, sub_batch)\n",
    "    sub_batch = tf.where(tf.math.is_nan(sub_batch), tf.zeros_like(sub_batch), sub_batch)\n",
    "    sub_batch = tf.cast(sub_batch,dtype=tf.float32)\n",
    "    tf_batch = tf.concat([tf_batch, sub_batch],0)\n",
    "    print(\"Data imported.\\n\")\n",
    "    \n",
    "    return tf_batch, tf.convert_to_tensor(y,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NB8BtK4fnX3V",
   "metadata": {
    "id": "NB8BtK4fnX3V"
   },
   "source": [
    "Importing 100,000 training events, defining the maximum number of particles per event to be 128. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81lICIHJ-d",
   "metadata": {
    "id": "ea81lICIHJ-d",
    "outputId": "4daffc4b-0fe8-4395-ca77-2c69c45ae4cf"
   },
   "outputs": [],
   "source": [
    "num_partons = 128\n",
    "training_data = import_top_data('train', dataset_size=250000,padding=num_partons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kgqyL8_pn6KG",
   "metadata": {
    "id": "kgqyL8_pn6KG"
   },
   "source": [
    "Function collating the pairwise kinematic functions, taking an arguement to define which features to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRZWNtVQ8PQ_",
   "metadata": {
    "id": "TRZWNtVQ8PQ_"
   },
   "outputs": [],
   "source": [
    "def to_m2_pair(x, eps=1e-8):\n",
    "    m2 = tf.square(x[:, :, :, 3:4]) - tf.reduce_sum(tf.square(x[:, :, :, :3]), axis=-1, keepdims=True)\n",
    "    #m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        m2 = tf.clip_by_value(m2, clip_value_min=eps, clip_value_max=10e32)\n",
    "    return m2\n",
    "\n",
    "def pairwise_lv_fts(input_data, output_vars=['kt', 'z', 'delta', 'm2'], eps=1e-8, for_onnx=False):\n",
    "\n",
    "    \n",
    "    x = input_data[:,:,:4]\n",
    "    batch_size = tf.squeeze(tf.shape(x))[0]\n",
    "    length = tf.squeeze(tf.shape(x))[1]\n",
    "    \n",
    "    num_outputs = len(output_vars)\n",
    "    \n",
    "    padding_bool = tf.reduce_all(x!=[0,0,0,0], axis=-1) \n",
    "    padding_arr = tf.cast(padding_bool, dtype=tf.float32)\n",
    "    padding_mat = tf.einsum('...i,...j->...ij', padding_arr, padding_arr)\n",
    "    outputs = []\n",
    "    \n",
    "    all_out = []\n",
    "\n",
    "    xi = tf.expand_dims(x,1)\n",
    "    xj = tf.expand_dims(x,2)\n",
    "    pt_rap  = to_ptrapphim_pre(x, False, eps=None, for_onnx=for_onnx)\n",
    "    pt, rap, phi = tf.split(to_ptrapphim_pre(x, False, eps=None, for_onnx=for_onnx), (1, 1, 1), axis=0)\n",
    "    \n",
    "    pt = tf.squeeze(pt)\n",
    "    rap = tf.squeeze(rap)\n",
    "    phi = tf.squeeze(phi)\n",
    "    \n",
    "\n",
    "    if batch_size == 1:\n",
    "        pt = tf.expand_dims(pt,0)\n",
    "        rap = tf.expand_dims(rap,0)\n",
    "        phi = tf.expand_dims(phi,0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    pti = tf.expand_dims(pt,1)\n",
    "    rapi = tf.expand_dims(rap,1)\n",
    "    phii = tf.expand_dims(phi,1)\n",
    "    pti = tf.expand_dims(pti,3)\n",
    "    rapi = tf.expand_dims(rapi,3)\n",
    "    phii = tf.expand_dims(phii,3)\n",
    "    \n",
    "    ptj = tf.expand_dims(pt,2)\n",
    "    rapj = tf.expand_dims(rap,2)\n",
    "    phij = tf.expand_dims(phi,2)\n",
    "    ptj = tf.expand_dims(ptj,3)\n",
    "    rapj = tf.expand_dims(rapj,3)\n",
    "    phij = tf.expand_dims(phij,3)\n",
    "    \n",
    "    \n",
    "    delta = tf.sqrt(delta_r2(rapi, phii, rapj, phij))\n",
    "    lndelta = tf.math.log(tf.clip_by_value(delta, clip_value_min=eps, clip_value_max=10e32))\n",
    "\n",
    "    ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else tf.math.minimum(pti, ptj)\n",
    "    lnkt = tf.clip_by_value(tf.math.log(ptmin * delta), clip_value_min=eps, clip_value_max=10e32)\n",
    "\n",
    "    ptmin_sum = tf.clip_by_value(ptmin / (pti + ptj), clip_value_min=eps, clip_value_max=10e32)\n",
    "    lnz = tf.math.sign(tf.math.log(ptmin_sum))*tf.clip_by_value(tf.abs(tf.math.log(ptmin_sum)), clip_value_min=eps, clip_value_max=10e32)        \n",
    "    ### Old lnz method lnz = tf.clip_by_value(tf.math.log(ptmin_sum), clip_value_min=eps, clip_value_max=10e32)        \n",
    "    \n",
    "    xij = xi + xj\n",
    "    lnm2 = tf.math.log(to_m2_pair(xij, eps=eps))\n",
    "\n",
    "    kinematic_dict =  dict([('kt', lnkt) ,('z', lnz) ,('delta', lndelta) , ('m2', lnm2)])\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        outputs.append(kinematic_dict[output_vars[i]])\n",
    "\n",
    "    y = tf.stack(outputs)\n",
    "    y = tf.squeeze(y)\n",
    "    \n",
    "    if num_outputs == 1:\n",
    "        y = tf.expand_dims(y, 0)\n",
    "\n",
    "    if batch_size == 1:\n",
    "        y = tf.expand_dims(y, 0)\n",
    "    \n",
    "    y = tf.transpose(y, perm=(1,2,3,0))\n",
    "    out = tf.einsum('...ij,...ijk->...ijk', padding_mat, y)\n",
    "    out = tf.where(tf.math.is_nan(out), tf.zeros_like(out), out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FT0DXCkDocec",
   "metadata": {
    "id": "FT0DXCkDocec"
   },
   "source": [
    "## Embedding functions\n",
    "\n",
    "Defining single and particle pair embedding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YgeBns9tpChl",
   "metadata": {
    "id": "YgeBns9tpChl"
   },
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, input_dim, activation = 'gelu'):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_dim = input_dim\n",
    "        self.masking = tf.keras.layers.Masking()\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        layer_arr = []\n",
    "        for dim in d_model:\n",
    "            temp = [tf.keras.layers.LayerNormalization(), tf.keras.layers.Dense(dim,activation=activation)]\n",
    "            layer_arr.append(temp)\n",
    "        \n",
    "        self.layers = layer_arr\n",
    "        \n",
    "    def call(self, x):\n",
    "        #x = self.dense(x)\n",
    "        x = self.masking(x)\n",
    "        x = self.batchnorm(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer[0](x)\n",
    "            x = layer[1](x)\n",
    "        return x\n",
    "    \n",
    "class PairEmbed(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, output_vars, d_model, activation = 'gelu', eps=1e-8, for_onnx=False):\n",
    "        super().__init__()\n",
    "        self.for_onnx = for_onnx\n",
    "        self.pairwise_lv_fts = partial(pairwise_lv_fts, output_vars = output_vars, eps=eps, for_onnx = for_onnx)\n",
    "        self.masking = tf.keras.layers.Masking()\n",
    "        self.num_outputs = len(output_vars)\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        layers_arr = []\n",
    "        for dim in d_model:\n",
    "            temp = [tf.keras.layers.Conv1D(filters=dim,kernel_size=1),tf.keras.layers.BatchNormalization(), tf.keras.layers.Activation('gelu')]\n",
    "            layers_arr.append(temp)\n",
    "            \n",
    "        self.layers = layers_arr\n",
    "        self.output_dim = d_model[-1]\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (batch, v_dim, seq_len)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        n_particles = tf.shape(x)[1]\n",
    "        seq_len = tf.shape(x)[2]\n",
    "        x = self.masking(x)\n",
    "        x = self.pairwise_lv_fts(x)\n",
    "        x = tf.reshape(x, (batch_size, n_particles * n_particles, 1, self.num_outputs))\n",
    "        x = self.batchnorm(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer[0](x)\n",
    "            x = layer[1](x)\n",
    "            x = layer[2](x)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, n_particles, n_particles, self.output_dim))\n",
    "        x = self.masking(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E0K_tTYApDxR",
   "metadata": {
    "id": "E0K_tTYApDxR"
   },
   "source": [
    "## Defining attention layer classes\n",
    "\n",
    "The particle and class attention blocks are defined, inheriting from a parent BaseAttention class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4738762",
   "metadata": {
    "id": "a4738762"
   },
   "outputs": [],
   "source": [
    "class ScalingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.initial_value = 1.\n",
    "        self.learned = tf.Variable(self.initial_value, name = 'learned_scalar')\n",
    "    def call(self, x):\n",
    "        return self.learned*x\n",
    "    \n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    # Base attention class with member keras layers to be inherited\n",
    "\n",
    "    def __init__(self, embed_dim, key_dim, num_heads, dropout, **kwargs):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.gelu = tf.keras.layers.Activation(tf.keras.activations.gelu)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.linear = tf.keras.layers.Dense(embed_dim, activation=None)\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim = key_dim, dropout=dropout)\n",
    "        self.p_mha = KerasPMHA(num_heads=num_heads, key_dim = key_dim, dropout=dropout)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "class PAttentionBlock(BaseAttention):\n",
    "    # Particle attention block, using the PMHA layer defined using the keras implementation \n",
    "    # of multi-head attention. \n",
    "    \n",
    "    def call(self, x, context):\n",
    "        # x: single particle embedding, (Batch size, Num. Partons, Embedding Dimension)\n",
    "        # context: pairwise particle embedding, (Batch size, Num. Partons, Num. Partons, Embedding Dimension)\n",
    "        \n",
    "        x_context = x\n",
    "        x = self.layernorm(x)\n",
    "        x = self.p_mha(x,x,x,bias_mask=context)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([x, x_context])\n",
    "        \n",
    "        x_context = x\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([x, x_context])\n",
    "        return x\n",
    "    \n",
    "class PlainAttentionBlock(BaseAttention):\n",
    "    \n",
    "    # Plain attention block, identical to Particle block except for a standard\n",
    "    # MHA layer replacing the PMHA. \n",
    "    \n",
    "    def call(self, x):\n",
    "        # x: single particle embedding, (Batch size, Num. Partons, Embedding Dimension)\n",
    "        x_context = x\n",
    "        x = self.layernorm(x)\n",
    "        x = self.mha(x,x,x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([x, x_context])\n",
    "        \n",
    "        x_context = x\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([x, x_context])\n",
    "        return x\n",
    "\n",
    "class CAttentionBlock(BaseAttention):\n",
    "    # Class attention block, following the design outlined in the Particle\n",
    "    # Transformer paper\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x, x_cls):\n",
    "        # x: output of final Particle/Plain attention layer, (Batch size, Num. Partons, Embedding Dimension)\n",
    "        # x_cls: class token to extract features, (Batch size, 1, Embedding Dimension)\n",
    "        x = tf.concat((x_cls, x), axis=1)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.mha(x_cls, x, x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.add([x, x_cls])\n",
    "        \n",
    "        x_context = x\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.add([x, x_context])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UeBrIDyap9cb",
   "metadata": {
    "id": "UeBrIDyap9cb"
   },
   "source": [
    "## Defining the Particle Transformer\n",
    "\n",
    "Now the component attention layers are defined, the complete transformer model can be built. This is defined, along with a multi-layer perceptron layer for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6WN-_4AbpOHz",
   "metadata": {
    "id": "6WN-_4AbpOHz"
   },
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, MLP_params):\n",
    "        super().__init__()\n",
    "        layer_arr = [] \n",
    "        for out_dim, drop_rate in MLP_params:\n",
    "            temp = [tf.keras.layers.Dense(out_dim, activation=None),tf.keras.layers.Dropout(rate = drop_rate)]\n",
    "            layer_arr.append(temp)\n",
    "            \n",
    "        self.MLP_layers = layer_arr\n",
    "    def call(self, x):\n",
    "        # x: class token or single layer attention, (Batch size, ..., embedding dimension)\n",
    "        for layers in self.MLP_layers:\n",
    "            x = layers[0](x)\n",
    "            x = layers[1](x)\n",
    "        return x\n",
    "\n",
    "class ParticleTransformer(tf.keras.Model):\n",
    "    def __init__(self, d_model, d_pair_model, max_len, input_dim, output_vars, num_heads, N_p, N_c, output_dims, isPlain = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        d_pair_model.append(num_heads)\n",
    "        self.embed_dim = d_model[-1]\n",
    "        self.embed = Embedding(d_model, input_dim)\n",
    "        self.pair_embed = PairEmbed(max_len, output_vars, d_pair_model)\n",
    "        self.isPlain = isPlain\n",
    "        \n",
    "        pattention = []\n",
    "        if isPlain:\n",
    "            for i in range(N_p):\n",
    "                pattention.append(PlainAttentionBlock(embed_dim = self.embed_dim,key_dim=self.embed_dim, num_heads=num_heads, dropout=0.1))\n",
    "            self.particle_attention_arr = pattention\n",
    "        \n",
    "        else:\n",
    "            for i in range(N_p):\n",
    "                pattention.append(PAttentionBlock(embed_dim = self.embed_dim,key_dim=self.embed_dim, num_heads=num_heads, dropout=0.1))\n",
    "            self.particle_attention_arr = pattention\n",
    "        \n",
    "        cattention = []\n",
    "        for i in range(N_c):\n",
    "            cattention.append(CAttentionBlock(embed_dim = self.embed_dim,key_dim=self.embed_dim, num_heads=num_heads, dropout=0.0))\n",
    "        self.class_attention_arr = cattention\n",
    "\n",
    "        # Standard CS class token\n",
    "        self.cls_tkn = tf.Variable(name=\"class token\", trainable=True, initial_value = tf.random.normal(shape=(1,1,self.embed_dim),stddev=0.2))\n",
    "        # Alternative, feature-based token\n",
    "        self.token_layer = Embedding(d_model, 1)\n",
    "        \n",
    "        self.mlp = MLP(output_dims)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        self.sigmoid = tf.keras.layers.Activation(tf.keras.activations.sigmoid)\n",
    "        self.n_p = N_p\n",
    "        self.n_c = N_c\n",
    "        self.output_dims = output_dims\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Inputs: Tensor of single particle kinematic features for each jet event,\n",
    "        # (Batch size, Num. partons, Num. kinematic features)\n",
    "        # px, py, pz, energy\n",
    "        \n",
    "        x = self.embed(inputs)\n",
    "        if self.isPlain:\n",
    "            for block in self.particle_attention_arr:\n",
    "                x = block(x)\n",
    "        \n",
    "        else:\n",
    "            U = self.pair_embed(inputs)\n",
    "            \n",
    "            for block in self.particle_attention_arr:\n",
    "                x = block(x, U)\n",
    "\n",
    "        broadcast_shape = (tf.shape(x)[0], 1, 1)\n",
    "        class_tkn = self.cls_tkn\n",
    "        class_tkn = tf.tile(class_tkn, broadcast_shape)\n",
    "        \n",
    "        # Feature-based token\n",
    "#         p = inputs[:,:,:4]\n",
    "#         E_tot = tf.reduce_sum(p[:,:,3], axis=1)\n",
    "#         pz_tot = tf.reduce_sum(p[:,:,2], axis=1)\n",
    "#         E_T2 = tf.square(E_tot) - tf.square(pz_tot)\n",
    "#         E_T2 = tf.reshape(E_T2,broadcast_shape)\n",
    "#         class_tkn = self.token_layer(E_T2)\n",
    "\n",
    "        x_context = x        \n",
    "        x = class_tkn\n",
    "\n",
    "        for block in self.class_attention_arr:\n",
    "            x = block(x_context, x)\n",
    "\n",
    "        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[2]))  \n",
    "        x = self.mlp(x)\n",
    "        if self.output_dims[-1][0] == 1:\n",
    "            x = self.sigmoid(x)\n",
    "        else:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yaq5zaYFqmQr",
   "metadata": {
    "id": "Yaq5zaYFqmQr"
   },
   "source": [
    "## Defining a wrapper\n",
    "\n",
    "A complete wrapper is defined, with methods to test, train, and to save/load weights. \n",
    "\n",
    "Parameters of the Particle Transformer are defined upon initialization, including chosen pairwise features and the option to use the plain MHA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe1302",
   "metadata": {
    "id": "ecbe1302"
   },
   "outputs": [],
   "source": [
    "class ParticleTransformerWrapper():\n",
    "    def __init__(self, d_model, d_pair_model, max_number_partons, input_dim, pairwise_outputs, num_heads, N_p, N_c, output_dims, isPlain = False, train_filename = 'train', model_name = \"ParticleTransformer\",  dataset_size = 10000, **kwargs):\n",
    "        self.ParT = ParticleTransformer(d_model, d_pair_model, max_number_partons, input_dim, pairwise_outputs,  num_heads, N_p, N_c, output_dims, isPlain=isPlain, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.padding = max_number_partons\n",
    "        self.model_name = model_name\n",
    "        self.weight_filename = model_name\n",
    "        self.weight_log = []\n",
    "        self.val_acc_loss_log = []\n",
    "        self.is_built = False\n",
    "        self.out_dim = output_dims[-1][0]\n",
    "        \n",
    "    def build(self):\n",
    "        input_shape = (None, self.padding, self.input_dim)\n",
    "        RAdam = tfa.optimizers.RectifiedAdam(beta_1=0.95, beta_2=0.999, epsilon=0.00001, weight_decay=0.01)\n",
    "        Lookahead = tfa.optimizers.Lookahead(RAdam)\n",
    "        \n",
    "        if self.out_dim == 1:\n",
    "            loss_func = 'binary_crossentropy'\n",
    "        else: loss_func = 'categorical_crossentropy'\n",
    "        print(loss_func)\n",
    "        \n",
    "        self.ParT.compile(\n",
    "             loss=loss_func,\n",
    "             optimizer=Lookahead,\n",
    "             metrics=['Accuracy'],\n",
    "            )\n",
    "        self.ParT.build(input_shape)\n",
    "        self.is_built = True\n",
    "    \n",
    "    def summary(self):\n",
    "        if self.is_built:\n",
    "            return self.ParT.summary()\n",
    "        else:\n",
    "            print(\"This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.\")\n",
    "            return None\n",
    "    \n",
    "    def train(self, dataset, dynamic_lr = False, initial_lr=0.001, max_lr = 0.01, min_max_ratio = 0.1, rampup_epochs = 0,sustain_ratio = 0.7, early_stopping = True, print_summary = False, train_val_split = 0.3, metrics = ['Accuracy'], epochs = 20, batch_size = 8, learning_rate = 0.001):\n",
    "        # Training function, with options for a dynamic learning rate and early stopping\n",
    "        # Weight checkpoints are saved each epoch, if validation accuracy higher \n",
    "        # than previous epoch\n",
    "        print(\"Loading data...\\n\") \n",
    "        X, y = dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), shuffle=True, test_size=train_val_split)\n",
    "        X_train = tf.convert_to_tensor(X_train)\n",
    "        X_test = tf.convert_to_tensor(X_test)\n",
    "        y_train = tf.convert_to_tensor(y_train)\n",
    "        y_test = tf.convert_to_tensor(y_test)\n",
    "        print(\"Loaded data.\\nCompiling model...\\n\")\n",
    "        \n",
    "        RAdam = tfa.optimizers.RectifiedAdam(beta_1=0.95, beta_2=0.999, epsilon=0.00001)#, weight_decay=0.01)\n",
    "        Lookahead = tfa.optimizers.Lookahead(RAdam)\n",
    "        \n",
    "        \n",
    "        if self.out_dim == 1:\n",
    "            loss_func = 'binary_crossentropy'\n",
    "        else: loss_func = 'categorical_crossentropy'\n",
    "        \n",
    "        self.ParT.compile(\n",
    "             loss=loss_func,\n",
    "             optimizer=Lookahead,\n",
    "             metrics=metrics,\n",
    "            )\n",
    "\n",
    "        print(\"Model compiled.\\nBeginning training... \\n\")\n",
    "        \n",
    "        path = \"Checkpoints/{}\".format(self.model_name)\n",
    "        path = path+\"/cp-{epoch:04d}\"\n",
    "\n",
    "              \n",
    "        min_lr = min_max_ratio*max_lr\n",
    "\n",
    "        sustain_epochs = int(sustain_ratio*epochs)\n",
    "        exp_decay = np.exp(-1)\n",
    "        if rampup_epochs != 0:\n",
    "            grad = (max_lr - initial_lr)/rampup_epochs\n",
    "        else: grad=0\n",
    "        \n",
    "        def lrfn(epoch):\n",
    "            \n",
    "            if epoch < rampup_epochs:\n",
    "                return initial_lr + grad*epoch\n",
    "            elif epoch < sustain_epochs+rampup_epochs:\n",
    "                return max_lr\n",
    "            else:\n",
    "                return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
    "\n",
    "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True) \n",
    "\n",
    "        self.ParT.save_weights(path.format(epoch=0))\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path,\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_Accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "        \n",
    "        es_loss = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=5)\n",
    "        # log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "        if early_stopping and dynamic_lr:\n",
    "            \n",
    "            history = self.ParT.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[es_loss, lr_callback, cp_callback], validation_data=(X_test, y_test), shuffle=True)\n",
    "\n",
    "        if early_stopping:\n",
    "            history = self.ParT.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[es_loss, cp_callback], validation_data=(X_test, y_test), shuffle=True)\n",
    "\n",
    "        if dynamic_lr:\n",
    "            history = self.ParT.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[lr_callback, cp_callback], validation_data=(X_test, y_test), shuffle=True)\n",
    " \n",
    "        else:\n",
    "            history = self.ParT.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[cp_callback], validation_data=(X_test, y_test), shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "        self.save_weights()\n",
    "        self.is_built = True\n",
    "        \n",
    "        if print_summary == True:\n",
    "            print(self.ParT.summary())\n",
    "        \n",
    "        figure = histplot(history)\n",
    "        figure.show()\n",
    "        filename = self.model_name\n",
    "        isFile = os.path.isfile(\"Figures/{}.png\".format(filename))\n",
    "        i=1\n",
    "        while isFile:\n",
    "            filename = \"{}{}\".format(self.model_name, i)\n",
    "            i+=1\n",
    "            isFile = os.path.isfile(\"Figures/{}.png\".format(filename))\n",
    "\n",
    "        figure.write_image(\"Figures/{}.png\".format(filename), width=800, height=500)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def test(self, dataset, get_score=True, percent_of_total_shown=0.1, batch_size=8):\n",
    "        X, y = dataset\n",
    "        y_pred = self.ParT.predict(X, batch_size=batch_size)\n",
    "        if get_score:\n",
    "            try:\n",
    "                score = self.ParT.evaluate(X, y, batch_size=batch_size, verbose=1)\n",
    "                print('Test loss:', score[0])\n",
    "                print('Test accuracy:', score[1])\n",
    "            except(RuntimeError):\n",
    "                print(\"Cannot evaluate model without first compiling the model for training/testing\")\n",
    "\n",
    "        fig = make_subplots(rows=1, cols=1,subplot_titles=(['ROC']))\n",
    "        try:\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y, y_pred)\n",
    "            AUC = metrics.auc(fpr, tpr)\n",
    "            reduced_fpr = []\n",
    "            reduced_tpr = []\n",
    "            reduced_threshold = []\n",
    "            for i in range(len(fpr)):\n",
    "                if np.random.rand()<percent_of_total_shown and i<len(thresholds):\n",
    "                    reduced_fpr.append(fpr[i])\n",
    "                    reduced_tpr.append(tpr[i])\n",
    "                    reduced_threshold.append(thresholds[i])\n",
    "\n",
    "\n",
    "            fig.add_trace(go.Scatter(x=reduced_fpr, y=reduced_tpr,\n",
    "                                    mode='lines',\n",
    "                                    marker_line_color=\"midnightblue\", \n",
    "                                    text = reduced_threshold,\n",
    "                                    hovertemplate = 'Thresh: %{text:.3f}<extra></extra>',\n",
    "                                    ), row=1, col=1)\n",
    "            \n",
    "        except(ValueError):\n",
    "            print(\"Could not compute ROC due NaN values in y_pred\")\n",
    "            AUC = np.NaN\n",
    "        fig['layout']['xaxis1']['title']='FPR'\n",
    "        fig['layout']['yaxis1']['title']='TPR'\n",
    "        \n",
    "        return fig, AUC\n",
    "    \n",
    "    def save_weights(self):\n",
    "        i=1\n",
    "        filename = self.model_name\n",
    "        isFile = os.path.isfile(\"Weights/{}.index\".format(filename))\n",
    "        while isFile:\n",
    "            filename = \"{}{}\".format(self.model_name, i)\n",
    "            isFile = os.path.isfile(\"Weights/{}.index\".format(filename))\n",
    "            i+=1\n",
    "        \n",
    "        self.ParT.save_weights(\"Weights/{}\".format(filename))\n",
    "        self.weight_filename = filename\n",
    "    \n",
    "    def load_weights(self, filepath):\n",
    "        isFile = os.path.isfile(filepath+\".index\")\n",
    "        if isFile:\n",
    "            try:\n",
    "                self.ParT.load_weights(filepath)\n",
    "            except(...):\n",
    "                print(\"Error loading weights. Check 'filename'.data-00000-of-00001 exists in the weights folder.\")\n",
    "        else: print(\"{} does not appear to exist.\".format(filepath)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_qnGq3MTrCJg",
   "metadata": {
    "id": "_qnGq3MTrCJg"
   },
   "source": [
    "## Defining the transformers\n",
    "\n",
    "The 6 transformer versions of interest are defined, with 8 particle blocks and 2 class blocks, with 8 heads for each attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d726dc",
   "metadata": {
    "id": "e9d726dc",
    "outputId": "0d814fb4-1d2b-457a-b1a4-07a4db04ff5a"
   },
   "outputs": [],
   "source": [
    "ParT_Full = ParticleTransformerWrapper(d_model=[128, 512, 128], d_pair_model=[64,64,64], \n",
    "                                        max_number_partons=num_partons, input_dim=11, \n",
    "                                        pairwise_outputs=['kt', 'z', 'delta', 'm2'], \n",
    "                                        num_heads=8, N_p=8, N_c=2, output_dims=[[1,0]], \n",
    "                                        model_name='ParT_Full_run1')\n",
    "\n",
    "\n",
    "num_partons = 128\n",
    "training_data = import_top_data('train', dataset_size=250000,padding=num_partons)\n",
    "\n",
    "batch_size = 256\n",
    "sustain_ratio = 0.7\n",
    "epochs = 8\n",
    "\n",
    "ParT_Full.train(dataset=training_data, epochs=epochs, train_val_split=0.3, \n",
    "                batch_size=batch_size, dynamic_lr=True, max_lr=0.001, \n",
    "                sustain_ratio=sustain_ratio, early_stopping=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7x625nAJEXwc",
   "metadata": {
    "id": "7x625nAJEXwc",
    "outputId": "4e24b67e-d5c5-4c81-ec66-081c5c3a716e"
   },
   "outputs": [],
   "source": [
    "ParT_Full = ParticleTransformerWrapper(d_model=[128, 512, 128], d_pair_model=[64,64,64], \n",
    "                                        max_number_partons=num_partons, input_dim=11, \n",
    "                                        pairwise_outputs=['kt', 'z', 'delta', 'm2'], \n",
    "                                       num_heads=8, N_p=8, N_c=2, output_dims=[[1,0]], \n",
    "                                       model_name='ParT_Full')\n",
    "ParT_Plain = ParticleTransformerWrapper(d_model=[128, 512, 128], d_pair_model=[64,64,64], \n",
    "                                        max_number_partons=num_partons, input_dim=11, \n",
    "                                        pairwise_outputs=['kt', 'z', 'delta', 'm2'], \n",
    "                                        num_heads=8, N_p=8, N_c=2, output_dims=[[1,0]], \n",
    "                                        isPlain=True, model_name='ParT_Plain')\n",
    "\n",
    "ParT_m2 = ParticleTransformerWrapper(d_model=[128, 512, 128], d_pair_model=[64,64,64], \n",
    "                                        max_number_partons=num_partons, input_dim=11, \n",
    "                                        pairwise_outputs=['m2'], num_heads=8, N_p=8, N_c=2, \n",
    "                                        output_dims=[[1,0]], model_name='ParT_m2')\n",
    "\n",
    "ParT_delta = ParticleTransformerWrapper(d_model=[128, 512, 128], d_pair_model=[64,64,64], \n",
    "                                        max_number_partons=num_partons, input_dim=11, \n",
    "                                        pairwise_outputs=['delta'], num_heads=8, N_p=8, N_c=2, \n",
    "                                        output_dims=[[1,0]], model_name='ParT_delta')\n",
    "\n",
    "ParT_kt = ParticleTransformerWrapper(d_model=[128, 512, 128], d_pair_model=[64,64,64], \n",
    "                                        max_number_partons=num_partons, input_dim=11, \n",
    "                                        pairwise_outputs=['kt'], num_heads=8, N_p=8, N_c=2, \n",
    "                                        output_dims=[[1,0]], model_name='ParT_kt')\n",
    "\n",
    "ParT_z = ParticleTransformerWrapper(d_model=[128, 512, 128], d_pair_model=[64,64,64], \n",
    "                                        max_number_partons=num_partons, input_dim=11, \n",
    "                                        pairwise_outputs=['z'], num_heads=8, N_p=8, N_c=2, \n",
    "                                        output_dims=[[1,0]], model_name='ParT_z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LzmJevSCr_au",
   "metadata": {
    "id": "LzmJevSCr_au"
   },
   "source": [
    "Training each version for 8 epochs, with a learning rate of 0.001 for 6 epochs and exponentially decaying towards 0.0001 for 2 epochs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
